{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:11:02,370 - Created a temporary directory at /tmp/tmpoxzglzef\n",
      "2025-05-06 22:11:02,375 - Writing /tmp/tmpoxzglzef/_remote_module_non_scriptable.py\n",
      "MONAI version: 1.0.1\n",
      "Numpy version: 1.23.4\n",
      "Pytorch version: 1.13.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 8271a193229fe4437026185e218d5b06f7c8ce69\n",
      "MONAI __file__: /home1/zc348/anaconda3/envs/dl-hmc_2301c/lib/python3.9/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 4.0.2\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 9.2.0\n",
      "Tensorboard version: 2.11.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.14.1\n",
      "tqdm version: 4.64.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 1.4.3\n",
      "einops version: 0.6.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import monai\n",
    "from monai.transforms import \\\n",
    "    Compose, LoadImaged, AddChanneld, Orientationd, \\\n",
    "    Spacingd, \\\n",
    "    ToTensord,  \\\n",
    "    DataStatsd, \\\n",
    "    ToDeviced\n",
    "from monai.data import list_data_collate\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary\n",
    "monai.config.print_config()\n",
    "import sys\n",
    "# sys.path.append(r'/data16/private/zc348/project/DL_HMC_attention/mCT/util/python')\n",
    "sys.path.append(r'/data16/private/zc348/project/DL_HMC_attention/util/python')\n",
    "import vicra_toolbox\n",
    "import nibabel; nibabel.imageglobals.logger.setLevel(40)\n",
    "# New transforms \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "sys.path.append(r'../')\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "from dlhmc.transforms import (\n",
    "    CreateImageStack,\n",
    "    ComputeRelativeMotion,\n",
    "    RandSamplePET,\n",
    "    ComputeRelativeMotiond,\n",
    "    CreateImageStackd,\n",
    "    RandSamplePETd,\n",
    ")\n",
    "\n",
    "from dlhmc.utils.data import (\n",
    "    concatenate_vicra,\n",
    "    split_dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set_all=[]# all case name\n",
    "\n",
    "test_set=[]# testing case name\n",
    "\n",
    "tr_set = tr_set_all[0:100]\n",
    "from dataset_summary_toolbox import compute_delta_T \n",
    "from data_prep_toolbox import delta_T_magnitude, Relative_motion_A_to_B_12, build_legal_dataset, deal_dataframe, clean_df\n",
    "from sampling_toolbox import data_split_sample, add_T_deltaT\n",
    "\n",
    "# Get the summaries\n",
    "\n",
    "summaries, delta_T_all=compute_delta_T(['APP311'], tr_set, data_type = 'real')\n",
    "\n",
    "def calculate_beta(df):\n",
    "    nums = df['T'].to_numpy()\n",
    "    trans, rot = 0, 0\n",
    "    for array in range(len(nums)):\n",
    "        for i in range(3):\n",
    "            trans += abs(nums[array][i])\n",
    "        for j in range(3):\n",
    "            rot += abs(nums[array][3 + j])\n",
    "#     print(\"trans:\", trans, \"rot:\", rot)\n",
    "    beta = trans/(trans + rot)\n",
    "#     print('beta:', beta)\n",
    "    return beta\n",
    "\n",
    "num_subsample = 360 #1800 # Number of samples per subject (to be split between training, validation and testing sets)\n",
    "\n",
    "train_list=list()\n",
    "val_list=list()\n",
    "test_list=list()\n",
    "\n",
    "# Sample training, validation and testing sets and save them in lists\n",
    "for summary in summaries: \n",
    "    summary['VICRA']=concatenate_vicra(summary)\n",
    "\n",
    "    print('Dataset contains total {:,d} entries'.format(len(summary)))\n",
    "    df_train, df_val, df_test = split_dataset(summary, num_subsample=num_subsample, test_size_percent=0.1, validation_size_percent=0.1)\n",
    "    \n",
    "    \n",
    "    print('Dataset split into 3 subsets:')\n",
    "    print(' Train set size: {:,d}'.format(len(df_train)))\n",
    "    print(' Val set size: {:,d}'.format(len(df_val)))\n",
    "    print(' Test set size: {:,d}'.format(len(df_test)))\n",
    "    \n",
    "    train_dict={\n",
    "        'DataFrame':df_train\n",
    "    }\n",
    "\n",
    "    val_dict={\n",
    "        'DataFrame':df_val\n",
    "    }\n",
    "    \n",
    "    test_dict={\n",
    "        'DataFrame':df_test\n",
    "    }\n",
    "    \n",
    "    train_list.append(train_dict)\n",
    "    val_list.append(val_dict)\n",
    "    test_list.append(test_dict)\n",
    "\n",
    "\n",
    "    \n",
    "# SIZE = (64,64,64)\n",
    "SIZE = (32,32,32)\n",
    "\n",
    "image_key = \"ThreeD_Cloud\"\n",
    "meta_keys=['ScanStart','VICRA']\n",
    "\n",
    "# Need to format as a dictionary for input\n",
    "# sample_dict = {\n",
    "#     'DataFrame': df_train,\n",
    "    # 'PatientID': df_sample['PatientID'].unique()\n",
    "# }\n",
    "\n",
    "# Here, we only have one subject of data, so a small example, but more subjects can be added\n",
    "# to this list.\n",
    "# sample_data_dict = [sample_dict]\n",
    "\n",
    "pet_transforms = monai.transforms.Compose([\n",
    "    CreateImageStackd(keys=\"DataFrame\", image_key=image_key, spatial_size=SIZE),\n",
    "    RandSamplePETd(keys=image_key, meta_data_key=\"DataFrame\", meta_keys=meta_keys, num_samples=8), \n",
    "    ComputeRelativeMotiond(keys=['VICRA_ref','VICRA_mov'], output_key='VICRA_rel'),\n",
    "    monai.transforms.DeleteItemsd(keys=[\"DataFrame\", \"ScanStart\", \"VICRA_ref\", \"VICRA_mov\", image_key]),\n",
    "    monai.transforms.ToTensord(\n",
    "        keys=[\"ScanStart_ref\", \"ScanStart_mov\", \"VICRA_rel\"]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_ds = monai.data.PersistentDataset(\n",
    "    data=train_list, \n",
    "    transform=pet_transforms, \n",
    "    cache_dir=\"/data16/private/zc348/project/dev/100hrrt/cache/\"\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=12, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=monai.data.list_data_collate\n",
    ")\n",
    "\n",
    "val_ds = monai.data.PersistentDataset(\n",
    "    data=val_list, \n",
    "    transform=pet_transforms, \n",
    "    cache_dir=\"/data16/private/zc348/project/dev/100hrrt/cache/\"\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=12, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=monai.data.list_data_collate\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlhmc.utils import Relative_motion_A_to_B_12,RotTransMatrix_6Params\n",
    "\n",
    "def matrix_transformation(gt_reg):\n",
    "    i_6_list = list()\n",
    "    for i_12 in gt_reg:\n",
    "        i_6 = np.reshape(i_12,12)\n",
    "        i_6 = torch.from_numpy(RotTransMatrix_6Params(i_6,1))\n",
    "        i_6_list.append(i_6)\n",
    "        \n",
    "    gt_reg_6 = torch.stack(i_6_list).float()\n",
    "    # return gt_reg_6\n",
    "    return gt_reg_6.to(gt_reg.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(UNet, self).__init__()\n",
    "        #Encode\n",
    "        self.conv_encode1 = self.contracting_block(in_channels=in_channel, out_channels=32)\n",
    "        self.conv_maxpool1 = torch.nn.MaxPool3d(kernel_size=2)\n",
    "        self.conv_encode2 = self.contracting_block(32, 64)\n",
    "        self.conv_maxpool2 = torch.nn.MaxPool3d(kernel_size=2)\n",
    "        self.conv_encode3 = self.contracting_block(64, 128)\n",
    "        self.conv_maxpool3 = torch.nn.MaxPool3d(kernel_size=2)\n",
    "\n",
    "\n",
    "    def contracting_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        \"\"\"\n",
    "        This function creates one contracting block\n",
    "        \"\"\"\n",
    "        block = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            torch.nn.BatchNorm3d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv3d(kernel_size=kernel_size, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            torch.nn.BatchNorm3d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encode_block1 = self.conv_encode1(x)\n",
    "        encode_pool1 = self.conv_maxpool1(encode_block1)\n",
    "        encode_block2 = self.conv_encode2(encode_pool1)\n",
    "        encode_pool2 = self.conv_maxpool2(encode_block2)\n",
    "        \n",
    "        encode_block3 = self.conv_encode3(encode_pool2)\n",
    "        encode_pool3 = self.conv_maxpool3(encode_block3)\n",
    "\n",
    "\n",
    "        return encode_pool3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_attention(nn.Module):\n",
    "    def  __init__(self, all_channel=128, all_dim=4*4*4):\t\n",
    "\n",
    "        super(Cross_attention, self).__init__()\n",
    "        self.linear_e = nn.Linear(all_channel, all_channel,bias = False)\n",
    "        self.channel = all_channel\n",
    "        self.dim = all_dim\n",
    "        self.gate = nn.Conv3d(all_channel, 1, kernel_size  = 1, bias = False)\n",
    "        self.gate_s = nn.Sigmoid()\n",
    "        self.conv1 = nn.Conv3d(all_channel*1, all_channel, kernel_size=3, padding=1, bias = False)\n",
    "        self.conv2 = nn.Conv3d(all_channel*1, all_channel, kernel_size=3, padding=1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm3d(all_channel)\n",
    "        self.bn2 = nn.BatchNorm3d(all_channel)\n",
    "        self.prelu = nn.ReLU(inplace=True)\n",
    "        self.conv3d_7 = nn.Conv3d(in_channels=all_channel *2 , out_channels=all_channel, kernel_size=1, stride=(1, 1, 1), padding=0)\n",
    "        self.pathC_bn1 = nn.BatchNorm3d(all_channel*2)\n",
    "        self.conv3d_8 = nn.Conv3d(in_channels=all_channel, out_channels=all_channel//2, kernel_size=3, stride=(1, 1, 1), padding=1)\n",
    "        self.conv3d_9 = nn.Conv3d(in_channels=all_channel//2, out_channels=all_channel//8, kernel_size=3, stride=(1, 1, 1), padding=1)\n",
    "        self.pathC_bn2 = nn.BatchNorm3d(all_channel//8)\n",
    "\n",
    "        self.conva = nn.Conv3d(all_channel, all_channel, kernel_size=1, padding=0, bias = False)\n",
    "        self.convb = nn.Conv3d(all_channel, all_channel, kernel_size=1, padding=0, bias = False)\n",
    "    \n",
    "    \n",
    "\t\t\n",
    "    def forward(self, exemplar, query): \n",
    "        \n",
    "\t \n",
    "        fea_size = query.size()[2:]\t \n",
    "        \n",
    "        #### correlation matrix computation\n",
    "        exemplar_v =  self.convb(exemplar) \n",
    "        query_v =  self.convb(query) \n",
    "        exemplar_q =  self.conva(exemplar) \n",
    "        query_k =  self.conva(query) \n",
    "        exemplar_flat = exemplar_q.view(-1, self.channel, self.dim) #N,C,H*W\n",
    "        query_flat = query_k.view(-1, self.channel, self.dim)\n",
    "        exemplar_t = torch.transpose(exemplar_flat,1,2).contiguous()  #batch size x dim x num\n",
    "        A = torch.bmm(exemplar_t, query_flat)\n",
    "        A = F.softmax(A, dim = 1) \n",
    "        B = F.softmax(torch.transpose(A,1,2),dim=1)\n",
    "        query_att = torch.bmm(exemplar_v.view(-1, self.channel, self.dim) , A).contiguous() \n",
    "        exemplar_att = torch.bmm(query_v.view(-1, self.channel, self.dim), B).contiguous()\n",
    "        #####self-gate mechanism\n",
    "        input1_att = exemplar_att.view(-1, self.channel, fea_size[0], fea_size[1], fea_size[2])  \n",
    "        input2_att = query_att.view(-1, self.channel, fea_size[0], fea_size[1], fea_size[2])\n",
    "        input1_mask = self.gate(input1_att)\n",
    "        input2_mask = self.gate(input2_att)\n",
    "        input1_mask = self.gate_s(input1_mask)\n",
    "        input2_mask = self.gate_s(input2_mask)\n",
    "        input1_att = input1_att * input1_mask\n",
    "        input2_att = input2_att * input2_mask\n",
    "        input1_att = input1_att +  exemplar_v\n",
    "        input2_att = input2_att + query_v\n",
    "        ######Deep norm and fusion\n",
    "        input1_att  = self.conv1(input1_att )\n",
    "        input2_att  = self.conv2(input2_att ) \n",
    "        input1_att  = self.bn1(input1_att )\n",
    "        input2_att  = self.bn2(input2_att )\n",
    "        input1_att  = self.prelu(input1_att )\n",
    "        input2_att  = self.prelu(input2_att )\n",
    "\n",
    "        conv_input = torch.cat((input1_att, input2_att), 1)\n",
    "        x = self.pathC_bn1(conv_input)\n",
    "        x = self.conv3d_7(x)\n",
    "        x = self.prelu(x)\n",
    "        x = self.conv3d_8(x)\n",
    "        x = self.prelu(x)\n",
    "        x = self.conv3d_9(x)\n",
    "        x = self.pathC_bn2(x)\n",
    "\n",
    "        return x.view(x.size()[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_att_dataloader(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, dropout=0.3,img_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "             \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.feature_extractor = UNet(1,1)\n",
    "\n",
    "        self.coattention = Cross_attention()\n",
    "\n",
    "\n",
    "        self.regression_layers = torch.nn.Sequential(\n",
    "\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.Linear(128, 16),\n",
    "            torch.nn.Linear(16, 6),\n",
    "        )\n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        y1 = self.feature_extractor(x1)\n",
    "        y2 = self.feature_extractor(x2)\n",
    "        y = self.coattention(y1,y2)\n",
    "        y = self.regression_layers(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # set deterministic training for reproducibility\n",
    "        monai.utils.misc.set_determinism(seed=42)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1 = batch[\"ThreeD_Cloud_ref\"]\n",
    "        x2 = batch[\"ThreeD_Cloud_mov\"]\n",
    "        ref_time = batch['ScanStart_ref']\n",
    "        mov_time = batch['ScanStart_mov']\n",
    "        # x_t = torch.stack([ref_time, mov_time], dim=1)\n",
    "        gt_reg = batch[\"VICRA_rel\"].float()\n",
    "        y = self.forward(x1,x2)\n",
    "       \n",
    "        target_six = matrix_transformation(gt_reg)\n",
    "        loss = self.loss_function(y, target_six)   \n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        # Calculate the average loss\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        # Logging at the end of every epoch\n",
    "        self.logger.experiment.add_scalar('Train/Loss', avg_loss, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1 = batch[\"ThreeD_Cloud_ref\"]\n",
    "        x2 = batch[\"ThreeD_Cloud_mov\"]\n",
    "        ref_time = batch['ScanStart_ref']\n",
    "        mov_time = batch['ScanStart_mov']\n",
    "        x_t = torch.stack([ref_time, mov_time], dim=1)\n",
    "        gt_reg = batch[\"VICRA_rel\"].float()\n",
    "        y = self.forward(x1,x2)\n",
    "        target_six = matrix_transformation(gt_reg)\n",
    "        loss = self.loss_function(y, target_six)   \n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Calculate the average loss\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        # Logging at the end of every epoch\n",
    "        self.logger.experiment.add_scalar('Val/Loss', avg_loss, self.current_epoch)\n",
    "\n",
    "        # Log the value for model checkpoint saving\n",
    "        self.log('val_loss', avg_loss.item()) #added .item(), otherwise validation wouldn't work \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        total_params = list(self.feature_extractor.parameters()) + list(self.regression_layers.parameters()) + list(self.coattention.parameters())\n",
    "        opt = torch.optim.Adam(total_params, lr=5e-4)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=200, gamma=0.98),\n",
    "                     'name': 'Learning Rate'}\n",
    "        return [opt], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = cross_att_dataloader().to(device)\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "\n",
    "MODEL_PATH = os.path.join('.','saved_model')\n",
    "print('MODEL_PATH={}'.format(MODEL_PATH))\n",
    "\n",
    "\n",
    "# Set up loggers and checkpoints\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=os.path.join(MODEL_PATH,'logs')\n",
    ")\n",
    "\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=os.path.join(MODEL_PATH),\n",
    "    filename=\"PETRegNet-{epoch}-{val_loss:.3f}\",\n",
    "    monitor='val_loss',\n",
    "    save_last=True,\n",
    "    save_top_k=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Check for last checkpoint\n",
    "resume_checkpoint = None\n",
    "if os.path.exists(os.path.join(MODEL_PATH,'last.ckpt')):\n",
    "    resume_checkpoint = os.path.join(MODEL_PATH,'last.ckpt')\n",
    "\n",
    "# Initialise Lightning's trainer.\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=[1],\n",
    "    max_epochs=3000, #10k is generally enough for single-subjct studies\n",
    "    logger=tb_logger,\n",
    "    callbacks=[lr_monitor,checkpoint_callback],\n",
    "    num_sanity_val_steps=1,\n",
    "    check_val_every_n_epoch=20,\n",
    "#     resume_from_checkpoint=resume_checkpoint # deprecated since v1.5\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, train_loader, val_loader, ckpt_path=resume_checkpoint) # previously was trainer.fit(model, train_dataloader=tr_loader, val_dataloaders=val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-hmc_2301c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
